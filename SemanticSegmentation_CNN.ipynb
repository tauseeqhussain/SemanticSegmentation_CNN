{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7X8yOP0Ew/4k4uKiJpX44",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tauseeqhussain/SemanticSegmentation_CNN/blob/main/SemanticSegmentation_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from segmentation_utility.semantic_segmentation \\\n",
        "    import (create_semantic_segmentation_dataset, display_segmented_image, display_grayscale_array, plot_class_masks)\n",
        "#from segmentation_utility.semantic_segmentation import create_semantic_segmentation_dataset, display_segmented_image, display_grayscale_array, plot_class_masks\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from keras.layers import Conv2D, AveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tfnyzZ_Fse33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================= step 1: prepare datasets =============================\n",
        "tf.config.set_visible_devices([], 'GPU')  # CPU mode, comment this line to switch GPU mode\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()  # load MNIST data\n",
        "\n",
        "# set how many digits you are going to recognize and segment. If 3, then digits 0,1,2 will be used.\n",
        "n_classes = 3\n",
        "\n",
        "# preprocess MNIST data for semantic segmentation\n",
        "# train_x: training data, train_y: training labels, test_x: validation data, test_y: validation labels\n",
        "train_x, train_y, test_x, test_y = create_semantic_segmentation_dataset(num_train_samples=5000,\n",
        "                                                                        num_test_samples=1000,\n",
        "                                                                        image_shape=(60, 60),\n",
        "                                                                        num_classes=n_classes)\n",
        "\n",
        "# display one example, note the semantic data is randomly generated.\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "display_grayscale_array(train_x[0], ax=ax1, title='Input image')\n",
        "display_segmented_image(train_y[0], ax=ax2, title='Ground truth')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-Ug_gh2wtdqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "WVhQFiMTA-34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================= step 2: construct the model here =============================\n",
        "#model = tf.keras.Sequential([])  # construct your model according to the given Table\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(16, 5, padding='same', activation='relu', input_shape=(60, 60,1)),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    Conv2D(32, 3, padding='same', activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    Conv2DTranspose(64,3,strides=2, padding='same', activation='relu'),\n",
        "    Conv2DTranspose(3,3,strides=2, padding='same', activation='sigmoid')\n",
        "])\n",
        "#checking my model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "eLGHPA11-rPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================= step 3: compile and train =============================\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.01),loss=BinaryCrossentropy(from_logits=True),metrics=['accuracy'])\n",
        "#model.compile()  # set up your training hyperparameters, use Adam for optimizer and 0.01 as learning rate\n",
        "# use BinaryCrossentropy to compute loss\n",
        "# make sure metrics=['accuracy] is used in the model.compile()\n",
        "\n",
        "EPOCH = 10\n",
        "model_history = model.fit(x=train_x, y=train_y, validation_data=(test_x, test_y), epochs=EPOCH)"
      ],
      "metadata": {
        "id": "3gjf0B4yK9lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================= step 4: plot loss/accuracy history ============================\n",
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "acc = model_history.history['accuracy']  # make sure metrics=['accuracy] is used in the model.compile()\n",
        "val_acc = model_history.history['val_accuracy']  # make sure metrics=['accuracy] is used in the model.compile()\n",
        "\n",
        "epochs = range(EPOCH)\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, loss, 'r-', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b-', label='Validation loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training/Validation Loss')\n",
        "plt.ylim([0, 1])  # adjust y axis if your training/validation loss is above 1\n",
        "plt.legend(loc='upper right')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, acc, 'r--', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b--', label='Validation accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training/Validation Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "hEAux-prgu9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QyMCWnk_w1UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================= step 5: prediction =============================\n",
        "test_y_predicted = model.predict(test_x)\n",
        "\n",
        "for i in range(3):\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    display_grayscale_array(test_x[i], ax=ax1, title='Input image')\n",
        "    display_segmented_image(test_y[i], ax=ax2, title='Ground truth')\n",
        "    display_segmented_image(test_y_predicted[i], ax=ax3, title='Prediction', threshold=0.5)\n",
        "    plt.show()\n",
        "    # plot_class_masks(test_y[i], test_y_predicted[i], title='y target and y predicted sliced along the channel axis')\n"
      ],
      "metadata": {
        "id": "mtepb1jIteoB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}